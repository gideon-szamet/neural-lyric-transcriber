{"cells":[{"cell_type":"markdown","metadata":{"id":"jbv8I0BD2Otn"},"source":["# 03 — Second Model: Transfrmers + CTC\n","Status: **Underfit / failed to generalize**. Kept to document the path and justify later choices.\n","\n","**Summary:** Despite extensive reconfiguration and debugging, this attempt with Wav2Vec2 (pretrained from facebook) failed to converge, producing NaN-masked losses and underfitted outputs.  \n","After abandoning the unstable pretrained Wav2Vec2 weights, I trained the model architecture from scratch, which ran without NaN issues but suffered from underfitting due to limited data. As a result, the model failed to produce reliable transcriptions and was ultimately set aside.\n"]},{"cell_type":"markdown","metadata":{"id":"rzX8dkQds5ib"},"source":["# Prepare Environment"]},{"cell_type":"markdown","metadata":{"id":"7wnVpF6w5eQ_"},"source":["## 1. Mount Drive\n","Mount Google Drive (only needed in Colab).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11466,"status":"ok","timestamp":1757428513265,"user":{"displayName":"gid szamet","userId":"18383499103108275399"},"user_tz":-180},"id":"enOwvV2fs_9l","outputId":"69af70d0-446b-4ad3-d0d9-82462f8d1413"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"miUYUvc2eems"},"source":["## 2. Install Required Libraries\n"]},{"cell_type":"code","source":["try:\n","    import google.colab\n","    from IPython import get_ipython\n","    ip = get_ipython()\n","    ip.system(\"pip install fsspec==2023.6.0 --quiet\")\n","    ip.system(\"pip install transformers datasets librosa jiwer torchaudio --quiet\")\n","    ip.system(\"pip install wandb --quiet\")\n","except Exception:\n","    print(\"Skipping pip installs (not in Colab).\")\n"],"metadata":{"id":"21pIC7QFIC5p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5qHlvQWtMeA"},"source":["## 3. Import packages\n","Load all Python libraries used later.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkgpP2tsdz6Z"},"outputs":[],"source":["# Imports\n","import torch\n","import torchaudio\n","from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Trainer, TrainingArguments\n","from datasets import load_dataset, Dataset, DatasetDict\n","import pandas as pd\n","import numpy as np\n","import librosa\n","import random\n","import os\n","import re\n","from dataclasses import dataclass\n","from typing import Any, Dict, List, Union"]},{"cell_type":"markdown","metadata":{"id":"iEq_TwY9uAh4"},"source":["## 4. Set paths\n","Define project root and subfolders (change `ROOT` to your clone path)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dfdthms7VXs"},"outputs":[],"source":["# Cell — Config paths (Notebook 2: model training)\n","import os, sys\n","from pathlib import Path\n","\n","# detect if running in Colab\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","# CONFIG: project root folder\n","# Change this path to the folder where you cloned/downloaded the repo\n","ROOT = Path(\"/content/drive/MyDrive/GitHub/musdb18-asr-dl\") if IN_COLAB else Path.cwd()\n","\n","# canonical subfolders\n","DATA_RAW        = ROOT / \"data\" / \"raw\"\n","DATA_PROCESSED  = ROOT / \"data\" / \"processed\"\n","OUT_DIR         = ROOT / \"outputs\"\n","CHECKPOINTS_DIR = ROOT / \"checkpoints\"\n","LOGS_DIR        = ROOT / \"logs\"\n","RESULTS         = ROOT / 'results'\n","MODELS_DIR      = RESULTS / 'models' / 'Transformer_model'\n","RESULTS.mkdir(parents=True, exist_ok=True)\n","MODELS_DIR.mkdir(parents=True, exist_ok=True)\n","\n","\n","# Hugging Face cache (persistent if on Colab + Drive)\n","HF_CACHE = Path(\n","    os.environ.get(\n","        \"HF_CACHE\",\n","        \"/content/drive/MyDrive/hf_cache\" if IN_COLAB else (Path.home() / \".cache\" / \"huggingface\")\n","    )\n",")\n","os.environ[\"HF_HOME\"] = str(HF_CACHE)\n","os.environ[\"HF_DATASETS_CACHE\"] = str(HF_CACHE)\n","os.environ[\"TRANSFORMERS_CACHE\"] = str(HF_CACHE)\n","\n","# ensure dirs exist\n","for d in [DATA_RAW, DATA_PROCESSED, OUT_DIR, CHECKPOINTS_DIR, LOGS_DIR, HF_CACHE]:\n","    d.mkdir(parents=True, exist_ok=True)\n","\n","# quick printout\n","print(\"ROOT         :\", ROOT)\n","print(\"DATA_RAW     :\", DATA_RAW)\n","print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n","print(\"OUT_DIR      :\", OUT_DIR)\n","print(\"CHECKPOINTS  :\", CHECKPOINTS_DIR)\n","print(\"LOGS_DIR     :\", LOGS_DIR)\n","print(\"HF_CACHE     :\", HF_CACHE)\n"]},{"cell_type":"markdown","metadata":{"id":"_gRdbN3S4tst"},"source":["# Data Process"]},{"cell_type":"markdown","metadata":{"id":"DsVrttMhf71j"},"source":["## Load and Prepare Chunked Audio + Text Data\n","load the vocal-only chunk metadata (with aligned lyrics), filter out non-lyric segments, and prepare a HuggingFace Dataset for training.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kPT8Q5ekf9Ge"},"outputs":[],"source":["csv_path = str(DATA_PROCESSED / 'train_segments_vocal_combined.csv')\n","\n","df = pd.read_csv(csv_path)\n","df = df[df['Lyric'].notnull() & (df['Lyric'].str.strip() != '')]  # remove empty targets\n","\n","# Shuffle and split\n","df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n","split_idx = int(len(df) * 0.9)\n","train_df, val_df = df[:split_idx], df[split_idx:]\n","\n","# Convert to HuggingFace Datasets\n","train_dataset = Dataset.from_pandas(train_df)\n","val_dataset = Dataset.from_pandas(val_df)\n","dataset = DatasetDict({\"train\": train_dataset, \"validation\": val_dataset})\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_ZjH5CZdD1BL"},"source":["### Load Wav2Vec2 model + processor\n","\n","Using facebook/wav2vec2-base-960h as base model.  \n","Freeze feature encoder so only the CTC head trains.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MXfZOH0_J1B"},"outputs":[],"source":["# Clean up cached models/datasets (mainly useful on Colab)\n","try:\n","    import google.colab  # Only run in Colab\n","    from IPython import get_ipython\n","    ip = get_ipython()\n","\n","    # HuggingFace model cache\n","    ip.system(\"rm -rf ~/.cache/huggingface/hub/models--facebook--wav2vec2-base-960h\")\n","\n","    # Temporary Colab dataset artifacts\n","    ip.system(\"rm -rf /content/tokenized_bpe_dataset\")\n","    ip.system(\"rm -rf /content/vocab.json\")\n","except Exception:\n","    print(\"Skipping Colab cleanup outside Colab.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FVCqWAGSmBkv"},"outputs":[],"source":["from transformers import Wav2Vec2Processor, Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor\n","\n","# Rebuild processor\n","tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n","\n","# ✅ Add <ctc_blank> token again\n","processor.tokenizer.add_tokens([\"<ctc_blank>\"])\n","blank_token_id = len(processor.tokenizer) - 1\n","print(\"✅ Processor loaded. Vocab size (with blank):\", len(processor.tokenizer))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1757428556946,"user":{"displayName":"gid szamet","userId":"18383499103108275399"},"user_tz":-180},"id":"t9f38Pd0gXkq","outputId":"d9834c0e-8515-4c65-a980-c9d64878903b"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Processor Vocab: {'<pad>': 0, '<s>': 1, '</s>': 2, '<unk>': 3, '|': 4, 'E': 5, 'T': 6, 'A': 7, 'O': 8, 'N': 9, 'I': 10, 'H': 11, 'S': 12, 'R': 13, 'D': 14, 'L': 15, 'U': 16, 'M': 17, 'W': 18, 'C': 19, 'F': 20, 'G': 21, 'Y': 22, 'P': 23, 'B': 24, 'V': 25, 'K': 26, \"'\": 27, 'X': 28, 'J': 29, 'Q': 30, 'Z': 31, '<ctc_blank>': 32}\n"]}],"source":["print(\"✅ Processor Vocab:\", processor.tokenizer.get_vocab())\n"]},{"cell_type":"markdown","metadata":{"id":"EdImiWTHDMWs"},"source":["### Preprocess text and audio\n","\n","- Remove special characters from lyrics  \n","- Load and resample each .wav file to 16kHz mono  \n","- Add \"speech\" and cleaned \"target_text\" fields\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJUQZ6mJW2ZH"},"outputs":[],"source":["### Text Cleaning Function\n","\n","# normalizes the lyric text to match the format expected by the pretrained Wav2Vec2 tokenizer:\n","\n","# Removes all punctuation except apostrophes (`'`)\n","# Converts all characters to uppercase (since the tokenizer uses uppercase letters)\n","# Replaces spaces with the `|` character (used as the word delimiter token in the tokenizer vocabulary)\n","\n","def clean_text(text):\n","    text = re.sub(r\"[^\\w\\s']\", '', text)  # Remove punctuation except apostrophes\n","    text = text.upper().strip()\n","    return text.replace(\" \", \"|\")\n"]},{"cell_type":"markdown","source":["### Convert audio chunks to arrays\n","\n","This function loads each audio chunk with **torchaudio**, converts stereo signals to mono, and ensures the waveform is stored as a clean 1-D NumPy array.  \n","For each chunk it adds:\n","- `speech`: the waveform (float32 array)  \n","- `sampling_rate`: the audio sampling rate  \n","- `target_text`: the cleaned lyric text  "],"metadata":{"id":"lFBBG5kW4Gwx"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTIVLplFhRbX"},"outputs":[],"source":["\n","def speech_file_to_array_fn(batch):\n","    try:\n","        speech_tensor, sr = torchaudio.load(batch[\"chunk_path\"])\n","\n","        # If stereo, average to mono\n","        if speech_tensor.shape[0] > 1:\n","            speech_tensor = torch.mean(speech_tensor, dim=0, keepdim=True)\n","\n","        waveform = speech_tensor.squeeze().numpy()\n","\n","        # Ensure waveform is a 1D array, not a scalar\n","        if isinstance(waveform, float) or np.isscalar(waveform) or waveform.ndim == 0:\n","            raise ValueError(\"Waveform is scalar\")\n","\n","        batch[\"speech\"] = waveform.astype(np.float32)  # Ensure consistent dtype\n","        batch[\"sampling_rate\"] = sr\n","        batch[\"target_text\"] = clean_text(batch[\"Lyric\"])\n","\n","        return batch\n","\n","    except Exception as e:\n","        print(f\"⚠️ Skipping file: {batch['chunk_path']} due to error: {e}\")\n","        return {\n","            \"speech\": np.zeros(1, dtype=np.float32),\n","            \"sampling_rate\": 16000,\n","            \"target_text\": \"\"\n","        }\n","\n","\n","dataset = dataset.map(speech_file_to_array_fn, num_proc=8)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPLeljnWB49j"},"outputs":[],"source":["from datasets import DatasetDict\n","\n","# Save to directory on Google Drive\n","dataset.save_to_disk(str(MODELS_DIR / 'untokenized_vocal_dataset'))"]},{"cell_type":"markdown","metadata":{"id":"E6S7_nWrJlX3"},"source":["### Tokenize inputs and labels\n","\n","This step converts raw audio and text into model-ready features:\n","\n","- **Waveform handling**: ensures each `speech` sample is a mono tensor at 16 kHz.  \n","- **Feature extraction**: applies the Wav2Vec2 processor to produce `input_values` and an `attention_mask`.  \n","- **Label encoding**: tokenizes the cleaned lyric text into `labels`.  \n","\n","The function is mapped across the dataset, creating a fully tokenized version that is then saved to disk for reuse.  \n"]},{"cell_type":"code","source":["# --- Load previously saved untokenized dataset ---\n","\n","from datasets import load_from_disk\n","import shutil\n","\n","try:\n","    import google.colab  # Colab-only copy step\n","    # Copy from Drive to Colab local scratch space\n","    shutil.copytree(\n","        MODELS_DIR / \"untokenized_vocal_dataset\",\n","        Path(\"/content/untokenized_vocal_dataset\"),\n","        dirs_exist_ok=True\n","    )\n","    dataset = load_from_disk(\"/content/untokenized_vocal_dataset\")\n","    print(\"Loaded dataset from Colab local copy.\")\n","except Exception:\n","    # Fallback: load directly from MODELS_DIR (works locally)\n","    dataset = load_from_disk(str(MODELS_DIR / \"untokenized_vocal_dataset\"))\n","    print(\"Loaded dataset directly from MODELS_DIR.\")\n"],"metadata":{"id":"R4LfJM7b4A7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgbzPvH0JtZ_"},"outputs":[],"source":["from datasets import load_from_disk\n","from transformers import Wav2Vec2Processor\n","import torch\n","import torchaudio\n","import numpy as np\n","import re\n","\n","\n","# Preprocessing function\n","def preprocess(example):\n","    # Convert waveform to tensor if needed\n","    waveform = example[\"speech\"]\n","    if isinstance(waveform, list):\n","        waveform = np.array(waveform, dtype=np.float32)\n","    if isinstance(waveform, np.ndarray):\n","        waveform = torch.tensor(waveform)\n","    if waveform.ndim > 1 and waveform.shape[0] > 1:\n","        waveform = torch.mean(waveform, dim=0)\n","    if example[\"sampling_rate\"] != 16000:\n","        waveform = torchaudio.transforms.Resample(orig_freq=example[\"sampling_rate\"], new_freq=16000)(waveform)\n","\n","    # Convert waveform to input values (features)\n","    input_values = processor.feature_extractor(\n","        waveform.numpy(),\n","        sampling_rate=16000,\n","        return_attention_mask=True,\n","        return_tensors=\"pt\",\n","        padding=True\n","    )\n","\n","    # Encode labels using the tokenizer\n","    with processor.as_target_processor():\n","        labels = processor.tokenizer(clean_text(example[\"target_text\"])).input_ids\n","\n","    return {\n","        \"input_values\": input_values[\"input_values\"][0].numpy().tolist(),\n","        \"attention_mask\": input_values[\"attention_mask\"][0].numpy().tolist(),\n","        \"labels\": labels\n","    }\n","\n","# Apply to full dataset\n","tokenized_dataset = dataset.map(preprocess, remove_columns=dataset[\"train\"].column_names)\n","\n","# Save to disk\n","tokenized_dataset.save_to_disk(\"/content/tokenized_bpe_dataset\")\n"]},{"cell_type":"markdown","metadata":{"id":"xR7vi4jmu8cM"},"source":["## Order the data set by input lengt\n","\n","for efficient batches and more stable gradients\n","without setting group_by_length=True in the training_args, which slows down the initiatin of the model (by approx 13 minutes)..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s1XYD-VuvgiN"},"outputs":[],"source":["def compute_input_length(example):\n","    return {\"input_length\": len(example[\"input_values\"])}  # return dict, not int\n","\n","# Map over dataset to get input lengths\n","tokenized_dataset = tokenized_dataset.map(\n","    compute_input_length,\n","    desc=\"🔍 Computing input lengths\",\n","    num_proc=8,\n",")\n","\n","# Sort each split by input length\n","tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].sort(\"input_length\")\n","tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].sort(\"input_length\")\n","\n","# Save to disk\n","tokenized_dataset.save_to_disk(\"/content/sorted_tokenized_vocal_dataset\")\n"]},{"cell_type":"markdown","metadata":{"id":"ff9hcBi6Ug6M"},"source":["### Save the dataset to drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sqcakzp-Ugkr"},"outputs":[],"source":["# --- Save tokenized dataset ---\n","import shutil\n","\n","save_path = MODELS_DIR / \"tokenized_vocal_dataset\"\n","\n","# Delete the folder if it already exists\n","if save_path.exists():\n","    shutil.rmtree(save_path)\n","\n","# Save newest version\n","tokenized_dataset.save_to_disk(str(save_path))\n","print(\"Tokenized dataset saved to:\", save_path)\n"]},{"cell_type":"markdown","metadata":{"id":"UWqxYC7FVHYz"},"source":["### Sanity check the tokenized dataset\n","\n","Inspect a few random samples to verify:\n","- `input_values` are waveform feature arrays (float32, ~length 1000–5000)\n","- `attention_mask` exists and matches input length\n","- `labels` are lists of integers (character token IDs)\n","- Label IDs can be decoded back into readable text\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ioNYFEJClSB"},"outputs":[],"source":["# --- Load previously saved tokenized dataset ---\n","\n","from datasets import load_from_disk\n","import shutil\n","\n","try:\n","    import google.colab  # Colab-only copy step\n","    # Copy from Drive (MODELS_DIR) to Colab local scratch space\n","    shutil.copytree(\n","        MODELS_DIR / \"tokenized_vocal_dataset\",\n","        Path(\"/content/tokenized_vocal_dataset\"),\n","        dirs_exist_ok=True\n","    )\n","    tokenized_dataset = load_from_disk(\"/content/tokenized_vocal_dataset\")\n","    print(\"Loaded tokenized dataset from Colab local copy.\")\n","except Exception:\n","    # Fallback: load directly from MODELS_DIR (works locally)\n","    tokenized_dataset = load_from_disk(str(MODELS_DIR / \"tokenized_vocal_dataset\"))\n","    print(\"Loaded tokenized dataset directly from MODELS_DIR.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v_AZXLupVQ7X"},"outputs":[],"source":["import random\n","from IPython.display import Audio\n","from datasets import load_from_disk\n","\n","# Sample 3 examples from the training set\n","for i in range(3):\n","    sample = tokenized_dataset[\"train\"][random.randint(0, len(tokenized_dataset[\"train\"]) - 1)]\n","\n","    print(f\"🗂 Sample {i+1}\")\n","    print(f\" - input_values: shape = {len(sample['input_values'])}, type = {type(sample['input_values'][0])}\")\n","    print(f\" - attention_mask: length = {len(sample['attention_mask'])}\")\n","    print(f\" - labels (IDs): {sample['labels']}\")\n","\n","    decoded = processor.decode(sample[\"labels\"], skip_special_tokens=True)\n","    print(f\" - Decoded text: {decoded}\")\n","\n","    print(\"-\" * 60)"]},{"cell_type":"markdown","metadata":{"id":"57SNOEEhRHWE"},"source":["# Model Training"]},{"cell_type":"markdown","metadata":{"id":"3wJ5IF00x1-3"},"source":["## Trainer Utilities"]},{"cell_type":"markdown","metadata":{"id":"-9VXyxfgYrAV"},"source":["### Standalone tokenizer Fix Cell"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61,"status":"ok","timestamp":1757430584616,"user":{"displayName":"gid szamet","userId":"18383499103108275399"},"user_tz":-180},"id":"_I10OPj7YqUS","outputId":"27c8a729-88b9-4b98-8dde-32fd9c342145"},"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Blank token added at ID: 32\n","✅ Updated vocab size: 33\n"]}],"source":["# === Add CTC blank token ===\n","processor.tokenizer.add_tokens([\"<ctc_blank>\"])\n","blank_token_id = len(processor.tokenizer) - 1\n","print(\"✅ Blank token added at ID:\", blank_token_id)\n","\n","# === Update vocab size on processor ===\n","print(\"✅ Updated vocab size:\", len(processor.tokenizer))\n"]},{"cell_type":"markdown","metadata":{"id":"FAsYO6i9xjeB"},"source":["### Define compute_metrics using jiwer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1g2v2aoeq4hx"},"outputs":[],"source":["import jiwer\n","\n","def compute_metrics(pred):\n","    pred_logits = pred.predictions\n","    pred_ids = np.argmax(pred_logits, axis=-1)\n","\n","    # Replace -100 with pad token ID\n","    label_ids = pred.label_ids\n","    label_ids = np.where(label_ids != -100, processor.tokenizer.pad_token_id, label_ids)\n","\n","    # Decode using group_tokens=False to avoid over-collapsing repeated characters\n","    pred_str = processor.batch_decode(pred_ids, group_tokens=False)\n","    label_str = processor.batch_decode(label_ids, group_tokens=False)\n","\n","    # Normalize\n","    pred_str = [s.lower().strip() for s in pred_str]\n","    label_str = [s.lower().strip() for s in label_str]\n","\n","    # Filter out empty references (to avoid WER > 1 due to divide-by-zero)\n","    pred_str_filtered = []\n","    label_str_filtered = []\n","\n","    for pred, ref in zip(pred_str, label_str):\n","        if len(ref.strip()) > 0:\n","            pred_str_filtered.append(pred)\n","            label_str_filtered.append(ref)\n","\n","    if not label_str_filtered:\n","        return {\"wer\": 1.0, \"cer\": 1.0}\n","\n","    return {\n","        \"wer\": jiwer.wer(label_str_filtered, pred_str_filtered),\n","        \"cer\": jiwer.cer(label_str_filtered, pred_str_filtered)\n","    }\n"]},{"cell_type":"markdown","metadata":{"id":"5QIWp_-uDs3k"},"source":["### Define Data Collator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fyLUO0eE0h5"},"outputs":[],"source":["from dataclasses import dataclass\n","from typing import List, Dict, Union\n","import torch\n","from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n","\n","@dataclass\n","class DataCollatorCTCWithPadding:\n","    processor: Wav2Vec2Processor\n","    padding: Union[bool, str] = True\n","\n","    def __call__(self, features: List[Dict[str, Union[List[float], List[int]]]]) -> Dict[str, torch.Tensor]:\n","        # Pad input_values\n","        input_values = [torch.tensor(f[\"input_values\"], dtype=torch.float32).squeeze() for f in features]\n","        input_values_padded = torch.nn.utils.rnn.pad_sequence(input_values, batch_first=True, padding_value=0.0)\n","\n","        # Create attention mask\n","        attention_mask = torch.zeros_like(input_values_padded, dtype=torch.long)\n","        for i, iv in enumerate(input_values):\n","            attention_mask[i, :iv.shape[0]] = 1\n","\n","        # Pad labels using tokenizer\n","        labels = [f[\"labels\"] for f in features]\n","        label_batch = self.processor.tokenizer.pad(\n","            [{\"input_ids\": l} for l in labels],\n","            padding=self.padding,\n","            return_tensors=\"pt\"\n","        )\n","        label_ids = label_batch[\"input_ids\"]\n","        label_ids[label_ids == self.processor.tokenizer.pad_token_id] = -100\n","\n","        # # 🔍 DEBUGGING: compare label lengths to CTC input lengths\n","        # with torch.no_grad():\n","        #     dummy_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\").eval()\n","        #     dummy_input = input_values_padded[:1]  # just one sample\n","        #     logits = dummy_model(dummy_input).logits\n","        #     input_len = logits.shape[1]  # time dimension of logits\n","\n","        # print(\"🔍 CTC Input Length (timesteps):\", input_len)\n","        # for i, lbl in enumerate(label_ids):\n","        #     real_len = (lbl != -100).sum().item()\n","        #     print(f\"🔍 Label[{i}] length: {real_len} vs input_len: {input_len}\")\n","        #     print(f\"    Label IDs: {[id for id in lbl.tolist() if id != -100]}\")\n","\n","        return {\n","            \"input_values\": input_values_padded,       # shape: (B, T)\n","            \"attention_mask\": attention_mask,          # shape: (B, T), 1s and 0s\n","            \"labels\": label_ids                        # shape: (B, L), with -100 for padding\n","        }\n"]},{"cell_type":"markdown","metadata":{"id":"eO2LmaftNLDQ"},"source":["### Save To Drive Callback"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZBYnM4uBNUK3"},"outputs":[],"source":["from transformers import TrainerCallback\n","import os\n","\n","class SaveToDriveCallback(TrainerCallback):\n","    def __init__(self, base_drive_path, processor):\n","        self.base_drive_path = base_drive_path\n","        self.processor = processor\n","\n","    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n","        step = state.global_step\n","        output_dir = os.path.join(self.base_drive_path, f\"wav2vec2-ctc-vocal-step-{step}\")\n","        model.save_pretrained(output_dir)\n","        self.processor.save_pretrained(output_dir)\n","        print(f\"✅ Saved model and processor to: {output_dir}\")\n"]},{"cell_type":"markdown","metadata":{"id":"WRQxgxkHF2dY"},"source":["### Training configuration - Arguments and Trainer\n","\n","Set batch size, learning rate, save strategy, gradient accumulation  \n","Enable best model saving and FP16 training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QEFxCV4LjoA"},"outputs":[],"source":["from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    seed=42,\n","    output_dir=\"./wav2vec2-ctc-vocal\",\n","    group_by_length=False,  # false since i manually sorted the inputs by length, to avoid the 13 min sorting at model initiation\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    eval_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    eval_steps=112,\n","    save_steps=112,\n","    logging_strategy=\"steps\",\n","    logging_steps=10,\n","    logging_first_step=True,\n","    num_train_epochs=10,\n","    gradient_checkpointing=True,\n","    fp16=True,\n","    learning_rate=5e-4,  # high to escape collapse\n","    weight_decay=0.005,  # lasso regularization\n","    warmup_ratio=0.1,\n","    max_grad_norm=1.0, # gradiet clipping\n","    save_total_limit=3,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"wer\",\n","    greater_is_better=False,\n","    push_to_hub=False,\n","    dataloader_pin_memory=True,\n","    report_to=[],\n",")\n"]},{"cell_type":"markdown","source":["### Load dataset"],"metadata":{"id":"7x1j4W7OD0Tr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WX6c4k-swd_X"},"outputs":[],"source":["# --- Load tokenized dataset before training ---\n","\n","from datasets import load_from_disk\n","import shutil\n","\n","try:\n","    import google.colab  # Colab-only copy step\n","    # Copy from Drive (MODELS_DIR) to Colab local scratch space\n","    shutil.copytree(\n","        MODELS_DIR / \"tokenized_vocal_dataset\",\n","        Path(\"/content/tokenized_vocal_dataset\"),\n","        dirs_exist_ok=True\n","    )\n","    tokenized_dataset = load_from_disk(\"/content/tokenized_vocal_dataset\")\n","    print(\"Loaded tokenized dataset from Colab local copy.\")\n","except Exception:\n","    # Fallback: load directly from MODELS_DIR (works locally)\n","    tokenized_dataset = load_from_disk(str(MODELS_DIR / \"tokenized_vocal_dataset\"))\n","    print(\"Loaded tokenized dataset directly from MODELS_DIR.\")\n"]},{"cell_type":"markdown","metadata":{"id":"F-ULEOwTQp4X"},"source":["### quick sanity\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qum7_fkXMfQs"},"outputs":[],"source":["### Sanity Check Cell\n","\n","from transformers import Wav2Vec2ForCTC\n","import torch\n","\n","# Load tokenized dataset\n","tokenized = tokenized_dataset\n","\n","# Inspect one sample\n","sample = tokenized[\"train\"][0]\n","print(\"Sample keys:\", sample.keys())\n","print(\"Input shape:\", len(sample[\"input_values\"]))\n","print(\"Label IDs:\", sample[\"labels\"])\n","\n","# Initialize processor and model\n","processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n","model.lm_head = torch.nn.Linear(model.config.hidden_size, len(processor.tokenizer), bias=True).to(\"cuda\")\n","model = model.to(\"cuda\").eval()\n","\n","# Convert to tensor\n","input_tensor = torch.tensor(sample[\"input_values\"], dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n","\n","# Forward pass\n","with torch.no_grad():\n","    output = model(input_tensor)\n","    logits = output.logits\n","    pred_ids = torch.argmax(logits, dim=-1)\n","\n","# Decode\n","decoded = processor.batch_decode(pred_ids, group_tokens=False)\n","print(\"Decoded prediction:\", decoded[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z8ErhGZmW6Xc"},"outputs":[],"source":["from datasets import load_from_disk\n","\n","print(tokenized_dataset)\n","print(tokenized_dataset[\"train\"].column_names)\n"]},{"cell_type":"markdown","metadata":{"id":"HChXmo91F65R"},"source":["## Train\n","\n","Use HuggingFace Trainer with model, collator, processor  \n","This will log loss and save best checkpoint automatically\n","\n","initializes the Wav2Vec2-CTC model with the custom vocabulary,  \n","sets up the `Trainer` for training on the tokenized dataset, and runs the training loop.  \n","\n","Key steps:\n","- Configure the model with correct vocab size and `blank_token_id`.\n","- Replace and reinitialize the output layer (`lm_head`) for the new vocabulary.\n","- Remove unused dataset columns to avoid errors.\n","- Train the model with evaluation, metrics, and a callback that saves progress to Drive.\n","- Save the final model and tokenizer to `MODELS_DIR` for persistence."]},{"cell_type":"code","source":["import numpy as np\n","from transformers import Trainer, Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2Config\n","from transformers.utils import logging\n","import logging as py_logging\n","import os\n","import torch.nn as nn\n","import gc, torch\n","from IPython.core.interactiveshell import InteractiveShell\n","\n","# Disable external integrations\n","os.environ[\"WANDB_DISABLED\"] = \"true\"  # avoid wandb init\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"  # reduce CUDA fragmentation\n","\n","# Logging setup\n","logging.set_verbosity_info()\n","logger = logging.get_logger()\n","logger.setLevel(py_logging.INFO)\n","\n","# Notebook display config\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Use already prepared dataset\n","dataset = tokenized_dataset\n","\n","# Set model save paths\n","MODEL_SAVE_DIR = MODELS_DIR / \"wav2vec2-ctc-vocal\"\n","MODEL_SAVE_DIR.mkdir(parents=True, exist_ok=True)\n","\n","# Initialize model config with tokenizer vocab size\n","config = Wav2Vec2Config.from_pretrained(\n","    \"facebook/wav2vec2-base-960h\",\n","    ctc_loss_reduction=\"mean\",\n","    pad_token_id=processor.tokenizer.pad_token_id,\n","    vocab_size=len(processor.tokenizer),\n","    bos_token_id=processor.tokenizer.bos_token_id,\n","    eos_token_id=processor.tokenizer.eos_token_id,\n","    forced_decoder_ids=None,\n","    blank_token_id=blank_token_id,\n",")\n","\n","# Build model and resize output layer\n","model = Wav2Vec2ForCTC(config)\n","model.lm_head = nn.Linear(model.config.hidden_size, model.config.vocab_size, bias=True)\n","nn.init.xavier_uniform_(model.lm_head.weight)\n","nn.init.zeros_(model.lm_head.bias)\n","model.ctc_loss = nn.CTCLoss(blank=blank_token_id, zero_infinity=True, reduction=\"mean\")\n","\n","# Remove unused columns\n","dataset[\"train\"] = dataset[\"train\"].remove_columns([\"input_length\"])\n","dataset[\"validation\"] = dataset[\"validation\"].remove_columns([\"input_length\"])\n","\n","# Free memory\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","# Trainer setup\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=dataset[\"train\"],\n","    eval_dataset=dataset[\"validation\"],\n","    tokenizer=processor,\n","    data_collator=DataCollatorCTCWithPadding(processor=processor),\n","    compute_metrics=compute_metrics,\n","    callbacks=[SaveToDriveCallback(base_drive_path=MODEL_SAVE_DIR, processor=processor)],\n",")\n","\n","# Train\n","trainer.train()\n","\n","# --- Save final model and tokenizer ---\n","model.save_pretrained(str(MODEL_SAVE_DIR))\n","processor.save_pretrained(str(MODEL_SAVE_DIR))\n","\n","print(\"✅ Model and tokenizer saved to:\", MODEL_SAVE_DIR)\n"],"metadata":{"id":"4q2z5RqIANFn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"07O5eJb7QwUZ"},"source":["### Sample Evaluation on a Few Training Examples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sfHJ-vy6QwtK"},"outputs":[],"source":["import torch\n","import random\n","from jiwer import wer, cer\n","\n","model.eval()\n","\n","# Sample a few tokenized training examples\n","sample_batch = random.sample(list(dataset[\"train\"]), 5)\n","\n","for i, sample in enumerate(sample_batch):\n","    input_values = torch.tensor(sample[\"input_values\"]).unsqueeze(0)  # [1, T]\n","    label_ids = sample[\"labels\"]\n","\n","    with torch.no_grad():\n","        logits = model(input_values).logits\n","\n","    pred_ids = torch.argmax(logits, dim=-1)\n","    pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n","\n","    # Decode label ids\n","    label_ids_tensor = torch.tensor(label_ids)\n","    label_ids_tensor[label_ids_tensor == -100] = processor.tokenizer.pad_token_id\n","    target_text = processor.decode(label_ids_tensor, skip_special_tokens=True)\n","\n","    # Compute metrics\n","    sample_wer = wer(target_text, pred_text)\n","    sample_cer = cer(target_text, pred_text)\n","\n","    # Print results\n","    print(f\"\\n--- Sample {i+1} ---\")\n","    print(f\"GT   : {target_text}\")\n","    print(f\"PRED : {pred_text}\")\n","    print(f\"WER  : {sample_wer:.3f}\")\n","    print(f\"CER  : {sample_cer:.3f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"j2ZBRIpdde2H"},"source":["# Test\n"]},{"cell_type":"markdown","metadata":{"id":"PYRG6-DfWszl"},"source":["Load and filter test DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I90NwR69XQqq"},"outputs":[],"source":["# Load the preprocessed test CSV\n","test_df = pd.read_csv(DATA_PROCESSED / \"test_segments_chunked.csv\")\n","\n","# Keep only rows with real lyrics\n","test_df = test_df[test_df[\"has_lyrics\"] == True].reset_index(drop=True)\n","\n","# Add ID column (if needed)\n","if \"id\" not in test_df.columns:\n","    test_df[\"id\"] = test_df.index\n"]},{"cell_type":"markdown","metadata":{"id":"FhXmqRY-XTbo"},"source":["Convert test DataFrame to HuggingFace Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y2DOEwTFXWEL"},"outputs":[],"source":["# Extract relevant columns\n","def extract_subset(df):\n","    return {\n","        \"id\": df[\"id\"],\n","        \"chunk_path\": df[\"chunk_path\"],\n","        \"text\": df[\"Lyric\"]\n","    }\n","\n","raw_test = test_df.apply(extract_subset, axis=1, result_type=\"expand\")\n","raw_dataset = Dataset.from_pandas(raw_test)\n"]},{"cell_type":"markdown","metadata":{"id":"J1cz7xWIXYFV"},"source":["Attach audio loader to Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9uh3y31XZkK"},"outputs":[],"source":["# Cast chunk_path column to Audio type\n","raw_dataset = raw_dataset.cast_column(\"chunk_path\", Audio(sampling_rate=16000))\n","raw_dataset = raw_dataset.rename_column(\"chunk_path\", \"audio\")"]},{"cell_type":"markdown","metadata":{"id":"I6icB227XccF"},"source":["Load trained processor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1sNLMkWmXczi"},"outputs":[],"source":["processor = Wav2Vec2Processor.from_pretrained(\n","   MODELS_DIR / \"wav2vec2-ctc-vocal\"\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"SarKW0CUXeac"},"source":["Tokenize the test dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WrwMXQ4JXgWS"},"outputs":[],"source":["# Tokenize inputs and labels\n","def prepare_example(batch):\n","    audio_array = batch[\"audio\"][\"array\"]\n","    inputs = processor(audio_array, sampling_rate=16000)\n","    with processor.as_target_processor():\n","        labels = processor(batch[\"text\"]).input_ids\n","    batch[\"input_values\"] = inputs[\"input_values\"]\n","    batch[\"attention_mask\"] = inputs[\"attention_mask\"]\n","    batch[\"labels\"] = labels\n","    return batch\n","\n","processed_test = raw_dataset.map(\n","    prepare_example,\n","    remove_columns=[\"audio\", \"text\", \"id\"],\n","    desc=\"Tokenizing\"\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"LPCbeMUQXlXK"},"source":["Save tokenized test set to disk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBlYe6pFXl6q"},"outputs":[],"source":["processed_test.save_to_disk(\n","    DATA_PROCESSED / \"tokenized_test_dataset\"\n",")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"13T5iPb1cOmiIDoAcXo-lyZioh03t89ow","timestamp":1753097602501}],"gpuType":"A100","mount_file_id":"15uglYe4YPoo2vYQWGms-PmRv8Hpfl7Tx","authorship_tag":"ABX9TyNTjvPbUiQZ4MGTm6Tfl0Ec"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}